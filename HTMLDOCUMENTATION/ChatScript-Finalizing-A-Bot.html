<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="chatscript-finalizing-a-bot-manual">ChatScript Finalizing a Bot Manual</h1>
<p>Copyright Bruce Wilcox, mailto:gowilcox@gmail.com www.brilligunderstanding.com <br>Revision 8/18/2019 cs9.62</p>
<p>OK. You've written a bot. It sort of seems to work. Now, before releasing it, you should polish it. There are a bunch of tools to do this.</p>
<h2 id="verification-verify">Verification (<code>:verify</code>)</h2>
<p>When I write a topic, before every rejoinder and every responder, I put a sample input comment. This has #! as a prefix. E.g.,</p>
<pre><code>topic: ~mytopic (keyword)

t: This is my topic.
    #! who cares
    a: (who) I care.

#! do you love me
?: (do you love) Yes.

#! I hate food
s: (I * ~hate * food) Too bad.</code></pre>
<p>This serves two functions. First, it makes it easy to read a topic-- you don't have to interpret pattern code to see what is intended. Second, it allows the system to verify your code in various ways. It is the &quot;unit test&quot; of a rule. If you've annotated your topics in this way, you can issue :verify commands.</p>
<p>Typically I start with proving the patterns work everywhere.</p>
<pre><code>:verify pattern</code></pre>
<p>Then I confirm keywords to access topics are OK.</p>
<pre><code>:verify keyword</code></pre>
<p>Then I verify rules don't block each other in a topic.</p>
<pre><code>:verify blocking</code></pre>
<p>Thereafter I can just do :verify to cover all of those.</p>
<p>Then I do more exotic checks:</p>
<pre><code>:verify sampletopic</code></pre>
<p>or</p>
<pre><code> :verify sample</code></pre>
<p>will tell me if a sample would end up answered by some other topic. If I don't like the answer, then I need to fix that other topic's rule. Sampletopic just tells you if the rule was hijacked by a different topic. Sample tells you even if some other rule in the same topic hijacked you.</p>
<p>And I might do</p>
<pre><code>:verify gambit</code></pre>
<p>to see if the bot can answer questions it poses the user in gambits. You can also verify individual topics in various ways.</p>
<p>Here are more samples:</p>
<pre><code>:verify </code></pre>
<p>all topics, all ways but samples and gambits</p>
<pre><code>:verify ~topicname </code></pre>
<p>named topic, all ways but samples and gambits</p>
<pre><code>:verify ~to*</code></pre>
<p>all topics whose names start with ~to</p>
<pre><code>:verify keyword</code></pre>
<p>all topics doing keyword verification</p>
<pre><code>:verify ~topic pattern</code></pre>
<p>named topic, doing pattern verification</p>
<pre><code>:verify blocking</code></pre>
<p>all topics doing blocking verification</p>
<pre><code>:verify sample</code></pre>
<p>all topics doing sample verification</p>
<pre><code>:verify gambit</code></pre>
<p>all topics doing gambit verification</p>
<pre><code>:verify all</code></pre>
<p>all topics doing all verifications</p>
<p>Passing verification means that each topic is plausibly scripted.</p>
<p>To be clear how valuable <code>:verify</code> is, consider this example. We authored a demo bot for a company and they added a bunch of code themselves. It was a 5500 rule bot. When I got around to verifying it, the results were: 57 keyword complaints, 92 pattern complaints, 45 blocking complaints, and (after fixing all those other complaints) 250 sampletopic complaints.</p>
<p>The <code>:verify</code> command takes the topic name first (and optional) and then a series of keywords about what to do.</p>
<pre><code>:verify keyword</code></pre>
<p>For responders, does the sample input have any keywords from the topic. If not, there may be an issue. Maybe the sample input has some obvious topic-related word in it, which should be added to the keywords list. Maybe it's a responder you only want matching if the user is in the current topic. E.g.,</p>
<pre><code>#! Do you like swirl?
?: ( swirl) I love raspberry swirl</code></pre>
<p>can match inside an ice cream topic but you don't want it to react to Does her dress swirl?.</p>
<p>Or maybe the sample input has no keywords but you do want it findable from outside (E.g., an idiom), so you have to make it happen. When a responder fails this test, you have to either add a keyword to the topic, revise the sample input, add an idiom entry to send you to this topic, or tell the system to ignore keyword testing on that input.</p>
<p>You can suppress keyword testing by augmenting the comment on the sample input with <code>!K</code> or marking the topic as a whole <code>TOPIC_NOKEYWORDS</code></p>
<p><code>#!!K</code> this is input that does not get keyword tested.</p>
<pre><code>:verify pattern</code></pre>
<p>For responders and rejoinders, the system takes the sample input and tests to see if the pattern of the following rule actually would match the given input. Failing to match means the rule is either not written correctly for what you want it to do or you wrote a bad sample input and need to change it. This test SHOULD NOT fail when you are done. Either your sample is bad, your pattern is bad, or ChatScript is bad.</p>
<p>Rules that need variables set certain ways can do variable assigns (<code>$</code> or <code>_</code> or) at the end of the comment. You can also have more than one verification input line before a rule.</p>
<pre><code>#! I am male $gender = male
#! I hate males $gender = male
s: ($gender=male I * male) You are not my type</code></pre>
<p>For a pattern that starts with <code>@_3+</code>, you can set the position by an assignment naming the word to start at (0 â€¦ number of words in sentence).</p>
<pre><code>#! I am the leader @_3+=0</code></pre>
<p>If you want to suppress testing, add <code>!P</code> to the comment.</p>
<pre><code>#!!P This doesn&#39;t get pattern testing.</code></pre>
<p>If the pattern absolutely SHOULD fail the test, use F. As in <code>#!F</code> or <code>#!!FB</code>. If you want to suppress pattern and keyword testing, just use <code>K</code> and <code>P</code> in either order:</p>
<pre><code>#!!KP this gets neither testing.</code></pre>
<p>You can also test that the input does not match the pattern by using <code>#!!R</code> instead of <code>#!</code>, though unless you were writing engine diagnostic tests this would be worthless to you.</p>
<pre><code>:verify blocking</code></pre>
<p>Even if you can get into the topic from the outside and the pattern matches, perhaps some earlier rule in the topic can match and take control instead. This is called blocking. One normally tries to place more specific responders and rejoinders before more general ones. The below illustrates blocking for are your parents alive? The sentence will match the first rule before it ever reaches the second one.</p>
<pre><code>#! do you love your parents
?: ( &lt;&lt; you parent &gt;&gt;) I love my parents *** this rule triggers by mistake

#! are your parents alive
?: ( &lt;&lt; you parent alive &gt;&gt;) They are still living</code></pre>
<p>The above can be fixed by reordering, but sometimes the fix is to clarify the pattern of the earlier rule.</p>
<pre><code>#! do you love your parents
?: ( ![alive living dead] &lt;&lt; you parent &gt;&gt;) I love my parents

#! are your parents alive
?: ( &lt;&lt; you parent alive &gt;&gt;) They are still living</code></pre>
<p>Sometimes you intend blocking to happen and you just tell the system not to worry about it using <code>!B</code>. Or you can have the entire topic ignored by the topic control <code>TOPIC NOBLOCKING</code>.</p>
<pre><code>#! do you enjoy salmon?
?: ( &lt;&lt; you ~like salmon &gt;&gt;) I love salmon

#!!B do you relish salmon?
?: (&lt;&lt; you ~like salmon &gt;&gt;) I already told you I did.</code></pre>
<p>The blocking test presumes you are within the topic and says nothing about whether the rule could be reached if you were outside the topic. That's the job of the keyword test. And it only looks at your sample input. Interpreting your pattern can be way too difficult.</p>
<p>If <code>:trace</code> has been set non-zero, then tracing will be turned off during verification, but any rules that fail pattern verification will be immediately be rerun with tracing on so you can see why it failed.</p>
<pre><code>:verify gambit</code></pre>
<p>One principle we follow in designing our bots is that if a user is asked a question in a gambit, the bot had better be able to answer that same question if asked of it.</p>
<p>The gambit verification will read all your gambits and if it asks a question, will ask that question of your bot. To pass, the answer must come from that topic or leave the bot with that topic as the current topic.</p>
<p>You can stop a topic doing this by adding the flag <code>TOPIC_NOGAMBITS</code>. There is no way to suppress an individual gambit, though clearly there are some questions that are almost rhetorical and won't require your bot answer them. E.g., <em>Did you know that hawks fly faster than a model airplane?</em></p>
<pre><code>:verify sampletopic &amp; :verify sample</code></pre>
<p>Once you've cleaned up all the other verifications, you get to sample verification. It takes sample inputs of your responders and sees if the chatbot would end up at the corresponding rule if the user issued it from scratch. This means it would have to find the right topic and find the right rule. If it finds the right topic, it will usually pass if you have managed most of the blocking issues.</p>
<p>I start with <code>:verify sampletopic</code>, which only shows inputs that fail to reach the topic they came from. This is generally more serious. It may be perfectly acceptable that the input gets trapped in another topic. This sample result is merely advisory. But maybe the rule that trapped it should be moved into the tested topic. Or maybe its just fine. But if the output you got doesn't work for the input, you can go to the rule for the output you got and alter the pattern in some way that excludes it reacting inappropriately to the input.</p>
<p>Once that is cleaned up, <code>:verify sample</code> will include rules that fail to get back to the correct rule of the topic. Usually, if you've fixed blocking, this won't be a problem.</p>
<p>If you stick a rule into Harry's <code>~introductions</code> topic like:</p>
<pre><code>#! what are you
u: (what are you) I am a robot</code></pre>
<p>Then <code>:verify sample</code> will complain about the sample. It is saying that if you are not ALREADY in the topic <code>~introductions</code>, then the input you give will get answered by some other topic (quibble) and not by this topic. The reason this would be true is that there are no keywords in the <code>~introduction</code> topic that could allow it to be found if you are not already in this topic.</p>
<p>Normally, a question like <em>what are you</em> is something I would put in the <code>~keywordless</code> topic, because there is no natural keyword in the question and you could be asked that question at any time from any topic. If, however, you actually want this rule in <code>~introductions</code>, you can tell CS NOT to do the sample input test on it via</p>
<pre><code>#!!S What are you</code></pre>
<h2 id="changing-tokenization">Changing tokenization</h2>
<p><code>:verify</code> normally works on the current <code>$cs_token</code> value. But in my bots, I might process inputs using two different values, one initially and one later. You can optionally specify what tokenization to be using by naming a user variable first.</p>
<pre><code>:verify $mytokencontrol ~mytopic pattern</code></pre>
<h2 id="spelling">Spelling</h2>
<p>To ensure the outputs don't have spelling errors, I do a <code>:build</code> like this: <code>:build Harry</code> outputspell and then fix any spelling complaints that should be fixed.</p>
<h2 id="sizing">Sizing</h2>
<p>We generally adhere to a tweet limit (140 characters) and run <code>:abstract 140</code> to warn us of lines that are longer.</p>
<h3 id="listvariables">`:listvariables'</h3>
<p>Because variables in CS have only 1 data type (text string), CS does not have any predeclarations of variables. You just use them on the fly. Of course if you make a typo, you use that on the fly as well and it may not be what you are expecting. Hence :listvariables.</p>
<p>:listvariables analyzes your compiled code in the TOPIC folder and writes a file listing each instance of a variable you have. The file is TMP/variables.txt. By default it lists all sorts of variables but you can limit it by naming the kinds you want:</p>
<pre><code>all - all variables
local - $_ variables
global - $ and $$ variables
permanent - $ variables
transient - $$ variables
 ```
Each line of the file names the variable and puts a + after it if it was an assignment.
If you then sort this file (e.g. DOS sort command `sort TMP/variables.txt &gt; x.txt ) they 
are made suitable to then from CS run :mergelines x.txt  which merges all lines having the same starting word  so 
there is just 1 example of each line with a prefix count of how many instances of such lines were seen. 
If you then sort that result (eg sort TMP/tmp.txt y.txt) you get a file that 
gives you how many times a variable was used. Things used only once are highly suspect
(set or used without the corresponding other side).
Helps you see instances where maybe
you typed the misspelled your variable. 

### `:topicinfo ~topic how`

This displays all sorts of information about a topic including its keywords, how they
overlap with other topics, what rules exist and whether they are erased or not. You either
name the topic or you can just use `~`, which means the current rejoinder topic (if there is
one). You can also wildcard the name like `~co*` to see all topics that start with `~co`.

If &#39;how&#39; is omitted, you get everything. 
You can restrict things with a collection of how keywords. These include: 

| how | meaning|
|-----|--------|
|`keys`| to display the keywords
|`overlap`| to display the overlap with other topics&#39; 
|`keywords`&lt;br&gt;`gambits`&lt;br&gt;`rejoinders`&lt;br&gt;`responders`&lt;br&gt;`all` | to limit rules to some of those 
|`used`&lt;br&gt;`available`| to see only those rules meeting that criteria.

Of particular importance in finalization is the key overlap map. Keyword overlap is
particularly interesting. As you assign keywords to topics, at times you will probably get
excessive. 
Some topics will share keywords with other topics. For some things, this is
reasonable. &quot;quark&quot; is a fine keyword for a topic on cheese and one on astronomy. But
odds are &quot;family&quot; is not a great keyword for a topic on money. 

Often an extraneous keyword won&#39;t really matter, 
but if the system is looking for a topic to gambit based on &quot;family&quot;, 
you really don&#39;t want it distracted by a faulty reference to money. 
That is, you want to know what keywords are shared across topics and then you can decide if that&#39;s appropriate. 
Sometimes you are told a word but don&#39;t see it in the topic keywords. 
Use `:up` on that word to see how it intersects.

I use `:topicinfo` keys to generate a map across all topics. I then read the first column list
of keywords for each topic to see if they obviously should be in that topic or if they don&#39;t
really strongly imply that topic. If not, I try to remove them from it. 

Sometimes when I look at the list of topics that a key overlaps with, 
I find multiple topics with similar ideas.
Like a topc called `~cars` and one called `~automobiles`. 
Often they are candidates for merging the topics.


## `:describe` and `:list`

If you have used :describe to document all of your permanent variables (see advanced
CS, `:build`), then `:list` can be used to show undocumented and potentially erroneously
spelled permanent variables
</code></pre>
<p>:list $ ^ ~ _ @</p>
<pre><code>
will list the documented kinds of items (you name which ones you want, could be just $)
and for variables it will list be documented and undocumented.


## `:abstract`

There are several useful :abstract calls to do during finalization.

### `:abstract 100` 

If you want to adjust output of yours that would be too long for something
like a phone screen, you can ask :abstract to show you all rules whose output would likely exceed some limit (here 100). 
Without a topic name it does the entire system. With a topic name e.g.

    :abstract 100 ~topicname

it is restricted to that topic.  You can also use a wildcard like `~top*` to do all topics with `~top` at the start of the name. 
You can also provide a list of topics, like 

    :abstract ~topic1 ~topic2


### `:abstract censor ~mywords` 

will note all output which contains any words in `~mywords`.
Of course regular uses may also appear. The censor command looks for any words
referred to by the concept given.


### `:abstract spell`

will examine the outputs of all topics (or topic given) to find words whose spelling might be faulty. 
It&#39;s not a guarantee it is, but it can warn you about potential mistakes.
And `:abstract` is handy just to print out a human-readable copy of your bot, 
without all the ChatScript scripting mess. As part of that, 
you can add header comments that will appear in the abstract. These are done as:

    #!x whatever
    
I often subset gambits and responders in a topic by section, e.g.,

    #!x*** FAMILY COMPOSITION
    ...
    #!x*** FAMILY RELATIONSHIPS


### `:abstract story`
will display just topics and their gambits (+ rejoinders of them).

### `:abstract responder` 
will display topics and their responders (+ rejoinders of them).


### `:permanentvariables`

This gives you a list of all permanent variables found in your source. Not only is
documenting them a good idea, but you might want to confirm they are all legit (correctly
spelled) and reinitialized as appropriate.


## Regression

Having built a functioning chatbot with lots of topics, I like to ensure I don&#39;t damage old
material in the future, so I create a regression test.

There are two ways to make a regression test (and you can have multple tests). One is to
create a source file with a bunch of questions. To be safe from most randomization
effects, you should designate the user and :reset the bot. After doing a bunch of stuff
you can do :reset again, so additional inputs are immune from effects of the prior stuff.

You can see an example of my std Rose regression file in `REGRESS/chatbotquestions/loebner.txt`

It starts out like this:
</code></pre>
<p>:user test :reset # 2001 Chatterbox challenge How is it going? How old are you? Are you male or female? What is your favorite color? Are you a bot? What is the date? ... :reset</p>
<pre><code>
and then has more questions and more :resets. Eventually it has a `:quit` before a bunch of
stuff I&#39;m not wanting tested at present.

The alternative way is to start a fresh conversation as a new user and assuming it makes no
mistakes then stop. You need to be a completely new user because CS will read your log file and 
things like `:reset` don&#39;t clear it. You want to have only the conversation you just did in that log.
Now you can make a regression file by typing `:regress init test` (or whatever user name
you used from either mechanism). CS reads your log file and converts it into `TMP/regress.txt` or you can do
</code></pre>
<p>:regress init user outputname</p>
<pre><code>
to name where it goes.

The user can be a full file name or just the name of a user whose log is in the USERS
directory. The output name should be whereever you want to put the resulting regression
file. If omitted it defaults to `TMP/regress.txt`.
Thereafter, all you have to do (not from a server version) is
</code></pre>
<p>:regress outputname</p>
<pre><code>
And the system will retest your conversation. I usually tranfer the file over to a regression
directory and rename it in there.

Regression will note any differences. Inconsequential differences it will merely tally at
the end, more interesting differences which may or may not be correct show the
alternative outputs and info, and differences CS thinks are major are flagged. If you use</code></pre>
<p>:regress terse outputname</p>
<pre><code>
then the system will only note what it thinks are fatal differences.
If there are differences, maybe they are correct (you edited your code). But CS can detect
where you merely edited the output, or a pattern, or a label, or added or subtracted rules
nearby. So it is likely to tell you what you need to know.

The system will ask if you want to update your regression file, and if you answer &quot;yes&quot;, it
will rewrite the regression file with the updated test results. Do this if it detects minor
differences so it can help stay on track in the future. Do this if the major differences are in fact OK. 
Otherwise decline and go fix your code somewhere.
</code></pre>
<p>:regress batch outputname ``` Useful for batch applications, this form does not prompt you if stuff is found changed. Instead it causes ChatScript to exit (0) if regress passes, or exit(1) if it doesnt. ## Mobile size issues</p>
<p>Mobile apps embedding CS probably do not want the entire dictionary. CS ships with a much smaller dictionary supporting basic english (through grade 6) in the folder DICT/BASIC. For a mobile app, all you need is a DICT/ENGLISH folder with the dict.bin and fact.bin from the BASIC folder.</p>
<p>CS will add things it finds in your TOPIC folder into the dictionary when it loads, so you can supplement words (spelling and parts of speech from those). They will not, however, provide correct pos tagging on conjugations of words not in the basic dictionary. And spelling correction may or may not work properly for words not in the dictionary. And the wordnet ontology will not propogate data through words not in the dictionary into higher level mappings. Eg Doberman may not know it is a more refined word of dog (though the concept sets will work). If you find you need a better dictionary, for a fee I can give you a basic dictionary where all the words of your script patterns and concepts are also correctly mapped.</p>
<p>You can also reduce code size by declaring a bunch of DISCARDxxx defines (see start of SRC/common.h) to remove chunks of CS you don't need, like the ScriptCompiler or testing abilities.</p>
</body>
</html>
